{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Callable, Any\n",
    "\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import xmltodict\n",
    "\n",
    "import os\n",
    "\n",
    "from typing import Any, Callable, Optional, Union\n",
    "from lightning.pytorch.core.optimizer import LightningOptimizer\n",
    "\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pictures_w_people(path):\n",
    "    peoples = []\n",
    "    with open(path) as f:\n",
    "        for line in f.readlines():\n",
    "            path, flag = line.split()\n",
    "            if flag == \"1\":\n",
    "                peoples.append(path)\n",
    "    return peoples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(box_path):\n",
    "    boxes = []\n",
    "    if os.path.isfile(box_path):\n",
    "        x = xmltodict.parse(open(box_path).read())['annotation']\n",
    "        if 'object' in x:\n",
    "            objects = x['object']\n",
    "            \n",
    "            if isinstance(objects, dict):\n",
    "                boxes.append(list(map(float, objects[\"bndbox\"].values())))\n",
    "            elif isinstance(objects, list):\n",
    "                for box in objects:\n",
    "                    boxes.append(list(map(float, box[\"bndbox\"].values())))\n",
    "            else:\n",
    "                raise Exception(f\"{type(objects)}\")\n",
    "    boxes = torch.tensor(boxes)\n",
    "    labels = torch.ones(len(boxes), dtype=int)\n",
    "    return boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, subset_dir: Path) -> None:\n",
    "        super().__init__()\n",
    "        self.subset_dir = subset_dir\n",
    "        pic_w_ppl = get_pictures_w_people(subset_dir)\n",
    "        self.path = Path(\"./PeopleArt-master\")\n",
    "        self.images = [self.path / \"JPEGImages\" /pic_name for pic_name in pic_w_ppl]\n",
    "        # self.images = list((subset_dir / \"JPEGImages\").glob(\"**/*.jpg\"))\n",
    "        # self.images = [image for image in self.images if os.path.isfile(self.subset_dir / \"Annotations\" / image.parts[-2] / Path(str(image.parts[-1]) + \".xml\"))]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[Tensor, dict[str, Tensor]]:\n",
    "        # нужно вернуть пару\n",
    "        # тензор изображения C x W x H\n",
    "        # словарь с ключами boxes, masks, labels\n",
    "        image_path = self.images[index]\n",
    "        img = Image.open(image_path)\n",
    "        convert_tensor = transforms.ToTensor()\n",
    "        image = convert_tensor(img)\n",
    "        #image = torch.load(image_path).unsqueeze(0)  # (W, H) -> (1, W, H)\n",
    "        box_path = self.path / \"Annotations\" / image_path.parts[-2] / Path(str(image_path.parts[-1]) + \".xml\")\n",
    "        boxes, labels = parse_xml(box_path)\n",
    "        return image, dict(\n",
    "                boxes=boxes,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_collate_fn_t = Callable[[list[tuple[Tensor, Any]]], Any]\n",
    "\n",
    "class Datamodule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        testdir: Path,\n",
    "        traindir: Path,\n",
    "        valdir: Path,\n",
    "        batch_size: int,\n",
    "        transform: Callable[[Image.Image], Tensor] = transforms.ToTensor(),\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.testdir = testdir\n",
    "        self.traindir = traindir\n",
    "        self.valdir = valdir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # в этом методе можно сделать предварительную работу, например\n",
    "        # скачать данные, сделать тяжёлый препроцессинг\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def collate_fn(self) -> _collate_fn_t | None:\n",
    "        return lambda batch: tuple(zip(*batch))\n",
    "        \n",
    "    def setup(self, stage: str) -> None:\n",
    "        # аргумент `stage` будет приходить из модуля обучения Trainer\n",
    "        # на стадии обучения (fit) нам нужны оба датасета\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = ImageDataset(\n",
    "                self.traindir,\n",
    "            )\n",
    "            self.val_dataset = ImageDataset(\n",
    "                self.valdir,\n",
    "            )\n",
    "        # на стадии валидации (validate) - только тестовый\n",
    "        elif stage == \"validate\":\n",
    "            self.val_dataset = ImageDataset(\n",
    "                 self.valdir,\n",
    "            )\n",
    "        elif stage == \"test\":\n",
    "            self.test_dataset = ImageDataset(\n",
    "                 self.testdir,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # есть ещё стадии `test` и `predict`, но они нам не понадобятся\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lit(L.LightningModule):\n",
    "    def __init__(self, learning_rate: float) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.weights = SSDLite320_MobileNet_V3_Large_Weights.COCO_V1\n",
    "        self.model = ssdlite320_mobilenet_v3_large(weights=self.weights)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.image_transform = self.weights.transforms()\n",
    "        # self.train_metrics = create_classification_metrics(\n",
    "        #     num_classes=10, prefix=\"train_\"\n",
    "        # )\n",
    "        # self.val_metrics = create_classification_metrics(num_classes=10, prefix=\"val_\")\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[list[Tensor], list[dict[str, Tensor]]], batch_idx: int\n",
    "    ):\n",
    "        self.model.train()\n",
    "        # image_tensors = [self.image_transform(x) for x, _ in batch]\n",
    "        # labels = [y for _, y in batch]\n",
    "        predicts = self.model(batch[0], batch[1])\n",
    "        loss = 0.5 * predicts['bbox_regression'] + 0.5 * predicts['classification']\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, on_step=False)\n",
    "        \n",
    "        # self.log_dict(self.train_metrics, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ):\n",
    "        # image_tensors = [self.image_transform(x) for x, _ in batch]\n",
    "        # labels = [y for _, y in batch]\n",
    "        images, targets = batch\n",
    "        predicts = self.model(images)\n",
    "        metric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "        metric.update(predicts, targets)\n",
    "        map = metric.compute()['map']\n",
    "        self.log('val_map', map)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"map\": map,\n",
    "        }\n",
    "    \n",
    "    def test_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ):\n",
    "        \n",
    "        # image_tensors = [self.image_transform(x) for x, _ in batch]\n",
    "        # labels = [y for _, y in batch]\n",
    "    \n",
    "        images, targets = batch\n",
    "        predicts = self.model(images)\n",
    "        metric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "        metric.update(predicts, targets)\n",
    "        map = metric.compute()['map']\n",
    "        self.log('test_map', map)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"predicts\": predicts,\n",
    "            \"map\": map,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # давайте кроме оптимизатора создадим ещё расписание для шага оптимизации\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": torch.optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer, milestones=[5, 10, 15]\n",
    "            )\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aim.pytorch_lightning import AimLogger\n",
    "\n",
    "# logger = AimLogger(repo=\"logs\", experiment=\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_module = Lit(\n",
    "    learning_rate=0.0005,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=10,\n",
    "    limit_train_batches=100,\n",
    "    limit_val_batches=100,\n",
    "\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = Datamodule(\n",
    "    testdir=Path(\"./PeopleArt-master/Annotations/person_test.txt\"),\n",
    "    traindir=Path(\"./PeopleArt-master/Annotations/person_train.txt\"),\n",
    "    valdir=Path(\"./PeopleArt-master/Annotations/person_val.txt\"),\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\python\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Encountered more than 100 detections in a single image. This means that certain detections with the lowest scores will be ignored, that may have an undesirable impact on performance. Please consider adjusting the `max_detection_threshold` to suit your use case. To disable this warning, set attribute class `warn_on_many_detections=False`, after initializing the metric.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  11%|█         | 1/9 [00:18<02:28,  0.05it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 9/9 [02:12<00:00,  0.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_map          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2883700728416443     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_map         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2883700728416443    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_map': 0.2883700728416443}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.test(\n",
    "    model=lit_module,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type            | Params\n",
      "----------------------------------------------------\n",
      "0 | model           | SSD             | 3.4 M \n",
      "1 | image_transform | ObjectDetection | 0     \n",
      "----------------------------------------------------\n",
      "3.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.4 M     Total params\n",
      "13.760    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\python\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  22%|██▏       | 2/9 [00:07<00:25,  0.28it/s, v_num=8]"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.fit(\n",
    "    model=lit_module,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 9/9 [00:28<00:00,  0.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_map          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3749520480632782     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_map         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3749520480632782    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_map': 0.3749520480632782}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.test(\n",
    "    model=lit_module,\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
